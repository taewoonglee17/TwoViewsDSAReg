{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import registration_utils as rut\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch3d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image \n",
    "import imageio\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from skimage import img_as_ubyte\n",
    "from scipy.spatial.transform import Rotation\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex, look_at_view_transform, look_at_rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cuda device \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "    print(torch.cuda.is_available())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Reference Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the obj and ignore the textures and materials.\n",
    "verts, faces_idx, _ = load_obj(\"../data/input/dsa_mesh_yaxis_up.obj\")\n",
    "faces = faces_idx.verts_idx\n",
    "\n",
    "# Initialize each vertex to be white in color.\n",
    "verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "textures = TexturesVertex(verts_features=verts_rgb.to(device))\n",
    "\n",
    "dsa_mesh = Meshes(\n",
    "    verts=[verts.to(device)],\n",
    "    faces=[faces.to(device)],\n",
    "    textures=textures\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a perspective camera.\n",
    "cameras = FoVPerspectiveCameras(device=device)\n",
    "\n",
    "# Set parameters which control the opacity and the sharpness of edges\n",
    "blend_params = BlendParams(sigma=1e-7, gamma=1e-5)\n",
    "\n",
    "# Define the settings for rasterization and shading. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size= (800,800),\n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma,\n",
    "    faces_per_pixel=100,\n",
    "    bin_size=0\n",
    ")\n",
    "\n",
    "# Create a silhouette mesh renderer by composing a rasterizer and a shader.\n",
    "silhouette_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "# We will also create a Phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=(800,800),\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    "    bin_size=0 \n",
    ")\n",
    "# We can add a point light in front of the object.\n",
    "lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(device=device, cameras=cameras, lights=lights)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the viewpoint using spherical angles\n",
    "distance = 150.0  \n",
    "elevation = 0.0   \n",
    "azimuth = -90.0 \n",
    "\n",
    "# Get Rotational and Translation Matrices for AP based on:\n",
    "# camera position in spherical coordinates, \"at\" vector where the camera is looking at, and \"up\" which defines up direction of camera\n",
    "R_0, T_0 = look_at_view_transform(distance, elevation, azimuth, at=((0,0,0,),), up=((0,1,0,),), device=device)\n",
    "# Get Rotational and Translation Matrices for LAT based on:\n",
    "R2_0, T2_0 = look_at_view_transform(distance, elevation, azimuth+90.0, at=((0,0,0,),), up=((0,1,0,),), device=device)\n",
    "\n",
    "# Render the mesh providing the values of R and T (AP direction)\n",
    "silhouette = silhouette_renderer(meshes_world=dsa_mesh, R=R_0, T=T_0)\n",
    "image_ref = phong_renderer(meshes_world=dsa_mesh, R=R_0, T=T_0)\n",
    "\n",
    "silhouette = silhouette.cpu().numpy()\n",
    "image_ref = image_ref.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(silhouette.squeeze()[..., 3])  # only plot the alpha channel of the RGBA image\n",
    "plt.grid(False)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_ref.squeeze())\n",
    "plt.grid(False)\n",
    "\n",
    "# Render the mesh providing the values of R and T (LAT direction)\n",
    "silhouette2 = silhouette_renderer(meshes_world=dsa_mesh, R=R2_0, T=T2_0)\n",
    "image_ref2 = phong_renderer(meshes_world=dsa_mesh, R=R2_0, T=T2_0)\n",
    "\n",
    "silhouette2 = silhouette2.cpu().numpy()\n",
    "image_ref2 = image_ref2.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(silhouette2.squeeze()[..., 3])  # only plot the alpha channel of the RGBA image\n",
    "plt.grid(False)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_ref2.squeeze())\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, meshes, renderer, image_ref, image_ref2, camera_initial_position):\n",
    "\n",
    "        super().__init__()\n",
    "        self.meshes = meshes\n",
    "        self.device = meshes.device\n",
    "        self.renderer = renderer\n",
    "\n",
    "        self.camera1_weight = 1.0\n",
    "        self.camera2_weight = 0.0\n",
    "\n",
    "        self.loss_graph = []\n",
    "\n",
    "         # Get the silhouette of the reference RGB image by finding all non-white pixel values.\n",
    "        image_ref = (image_ref[..., :3].max(-1) != 1).astype(np.float32).squeeze()\n",
    "        # image_ref = rut.add_noise(image_ref, num_spots=1500, spot_size=5)\n",
    "        image_ref = torch.from_numpy(image_ref)\n",
    "        self.register_buffer('image_ref', image_ref)\n",
    "\n",
    "        image_ref2 = (image_ref2[..., :3].max(-1) != 1).astype(np.float32).squeeze()\n",
    "        # image_ref2 = rut.add_noise(image_ref2, num_spots=1500, spot_size=5)\n",
    "        image_ref2 = torch.from_numpy(image_ref2)\n",
    "        self.register_buffer('image_ref2', image_ref2)\n",
    "\n",
    "        # Create an optimizable parameter for the x, y, z position of the camera, at vector, and up vector\n",
    "        self.camera_position = nn.Parameter(\n",
    "            torch.from_numpy(np.array([\n",
    "                [camera_initial_position[0][0].item(), camera_initial_position[0][1].item(), camera_initial_position[0][2].item()],\n",
    "                [camera_initial_position[1][0].item(), camera_initial_position[1][1].item(), camera_initial_position[1][2].item()],\n",
    "                [camera_initial_position[2][0].item(), camera_initial_position[2][1].item(), camera_initial_position[2][2].item()]\n",
    "                ], dtype=np.float32)).to(meshes.device))\n",
    "\n",
    "    def forward(self):\n",
    "        # Render the image using the updated camera position. Based on the new position of the\n",
    "        # camera we calculate the rotation and translation matrices\n",
    "        # position, at, up\n",
    "        R = look_at_rotation(self.camera_position[0][None, :], \n",
    "                             self.camera_position[1][None, :],\n",
    "                             self.camera_position[2][None, :],device=self.device)\n",
    "        T = -torch.bmm(R.transpose(1, 2), self.camera_position[0][None, :, None])[:, :, 0]\n",
    "        \n",
    "        c1 = self.camera_position[0][None, :].squeeze()\n",
    "        at = self.camera_position[1][None, :].squeeze()\n",
    "        up = self.camera_position[2][None, :].squeeze()\n",
    "\n",
    "        # camera 2 position is rotated 90 degrees from camera 1\n",
    "        c2 = (at - torch.linalg.cross(up,at-c1))/torch.norm(at - torch.linalg.cross(up,at-c1)) * torch.norm(at-c1)\n",
    "\n",
    "        # Use 90 degree rotated position, and same at and up vectors as camera 1\n",
    "        R2 = look_at_rotation(c2[None, :],\n",
    "                            self.camera_position[1][None, :],\n",
    "                            self.camera_position[2][None, :], device=self.device)\n",
    "        T2 = -torch.bmm(R2.transpose(1, 2), c2[None, :, None])[:, :, 0] \n",
    "\n",
    "        image = self.renderer(meshes_world=self.meshes.clone(), R=R, T=T)\n",
    "        image2 = self.renderer(meshes_world=self.meshes.clone(), R=R2, T=T2)\n",
    "\n",
    "        # Calculate the silhouette loss\n",
    "        loss = torch.sqrt(torch.sum(self.camera1_weight*(image[..., 3] - self.image_ref) ** 2 +\n",
    "                                    self.camera2_weight*(image2[..., 3] - self.image_ref2) ** 2))\n",
    "        loss_first_camera = torch.sqrt(torch.sum(self.camera1_weight*(image[..., 3] - self.image_ref) ** 2))\n",
    "        self.loss_graph.append(loss_first_camera.item())\n",
    "        return loss, image, image2, loss_first_camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save images periodically and compose them into a GIF.\n",
    "filename_output = \"../data/output/synthetic/registration_synthetic_AP.gif\"\n",
    "writer = imageio.get_writer(filename_output, mode='I')\n",
    "filename_output2 = \"../data/output/synthetic/registration_synthetic_LAT.gif\"\n",
    "writer2 = imageio.get_writer(filename_output2, mode='I')\n",
    "\n",
    "first_camera_initial_position = torch.tensor([\n",
    "    [-150, 10, -30],\n",
    "    [10, 0, 5],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "# Initialize a model using the renderer, mesh and reference image\n",
    "model = Model(meshes=dsa_mesh,\n",
    "              renderer=silhouette_renderer,\n",
    "              image_ref=image_ref,\n",
    "              image_ref2=image_ref2,\n",
    "              camera_initial_position=first_camera_initial_position\n",
    "              ).to(device)\n",
    "\n",
    "# Create an optimizer. Here we are using Adam and we pass in the parameters of the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Starting Position and Reference Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "_, image_init, image_init2, _ = model()\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(image_init.detach().squeeze().cpu().numpy()[..., 3])\n",
    "plt.grid(False)\n",
    "plt.title(\"Starting position AP\")\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(model.image_ref.cpu().numpy().squeeze())\n",
    "plt.grid(False)\n",
    "plt.title(\"Reference silhouette AP\")\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(image_init2.detach().squeeze().cpu().numpy()[..., 3])\n",
    "plt.grid(False)\n",
    "plt.title(\"Starting position LAT\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(model.image_ref2.cpu().numpy().squeeze())\n",
    "plt.grid(False)\n",
    "plt.title(\"Reference silhouette LAT\")\n",
    "\n",
    "matplotlib.image.imsave(\"../data/output/synthetic/synthetic_image_ref.png\", model.image_ref.cpu().numpy().squeeze())\n",
    "matplotlib.image.imsave(\"../data/output/synthetic/synthetic_image_ref2.png\", model.image_ref2.cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving GIF\n",
    "strikes_to_break = 0\n",
    "strike_out = 10\n",
    "loss_previous1 = 0.0\n",
    "second_camera_added_i = 0\n",
    "second_registration_mode = True # True for both AP + LAT at the same time, False for AP then LAT\n",
    "if (second_registration_mode == True):\n",
    "    model.camera1_weight = 1.0\n",
    "    model.camera2_weight = 1.0\n",
    "total_iterations = 700\n",
    "index_for_gif = 0\n",
    "fontsize = 40\n",
    "\n",
    "# initializing file\n",
    "with open('../data/output/synthetic/RT_data_synthetic.txt', 'w') as f:\n",
    "\n",
    "    loop = tqdm(range(total_iterations))\n",
    "    for i in loop:\n",
    "\n",
    "        # Breaks if loss increases or loss difference is small strikes_to_break number of times\n",
    "        if i >= 11 and i % 10 ==0:\n",
    "            if  loss.item() - loss_previous1 > 10:\n",
    "                strikes_to_break += 1\n",
    "                print(\"strike \"+str(strikes_to_break)+\" because loss increased at i = \"+str(i)+\".\")\n",
    "\n",
    "            if abs(loss.item() - loss_previous1) < 0.05:\n",
    "                strikes_to_break += 1\n",
    "                print(\"strike \"+str(strikes_to_break)+\" because loss difference is small at i = \"+str(i)+\".\")\n",
    "\n",
    "        if second_registration_mode == True:\n",
    "            if strikes_to_break == strike_out:\n",
    "                break\n",
    "        else:\n",
    "            if strikes_to_break == strike_out or i == int(total_iterations/2):  \n",
    "                print(\"Adding second image to registration.\")\n",
    "                model.camera1_weight = 1.0\n",
    "                model.camera2_weight = 1.0\n",
    "                strikes_to_break = 0\n",
    "                second_registration_mode = True\n",
    "                second_camera_added_i = i\n",
    "            \n",
    "        if i >= 1 and i % 10 == 0:\n",
    "            loss_previous1 = loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, image_sillouette, image_sillouette2, loss_first_camera = model()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description('Optimizing (loss %.4f)' % loss.data)\n",
    "\n",
    "        # Save outputs to create a GIF.\n",
    "        if i % 10 == 0:\n",
    "            R = look_at_rotation(model.camera_position[0][None, :],\n",
    "                                model.camera_position[1][None, :],\n",
    "                                model.camera_position[2][None, :], device=model.device)\n",
    "            T = -torch.bmm(R.transpose(1, 2), model.camera_position[0][None, :, None])[:, :, 0]   # (1, 3)\n",
    "\n",
    "            image = phong_renderer(meshes_world=model.meshes.clone(), R=R, T=T)\n",
    "            image = image[0, ..., :3].detach().squeeze().cpu().numpy()\n",
    "            image = img_as_ubyte(image)\n",
    "\n",
    "            \n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(image_sillouette.detach().squeeze().cpu().numpy()[..., 3], cmap='viridis', alpha=0.5)\n",
    "            plt.imshow(model.image_ref.detach().squeeze().cpu().numpy(), cmap='hot', alpha=0.5)\n",
    "            plt.title(\"AP View\", size=fontsize)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # plt.savefig('../data/output/synthetic/temp_gif_frame_AP.png', bbox_inches='tight')  # Save to a temporary location\n",
    "            # writer.append_data(imageio.v2.imread('../data/output/synthetic/temp_gif_frame_AP.png'))  # Read and append to GIF\n",
    "\n",
    "            f.write(str(model.camera_position[0][None, :].cpu().detach().squeeze().tolist()))\n",
    "            f.write('\\n')\n",
    "            f.write(str(model.camera_position[1][None, :].cpu().detach().squeeze().tolist()))\n",
    "            f.write('\\n')\n",
    "            f.write(str(model.camera_position[2][None, :].cpu().detach().squeeze().tolist()))\n",
    "            f.write('\\n')\n",
    "\n",
    "            matplotlib.image.imsave(\n",
    "                \"../data/output/synthetic/image_sillouette_synthetic_\"+str(index_for_gif)+\".png\",\n",
    "                image_sillouette.detach().squeeze().cpu().numpy()[..., 3])\n",
    "\n",
    "            if(second_registration_mode):\n",
    "                c1 = model.camera_position[0][None, :].squeeze()\n",
    "                at = model.camera_position[1][None, :].squeeze()\n",
    "                up = model.camera_position[2][None, :].squeeze()\n",
    "\n",
    "                c2 = (at - torch.linalg.cross(up,at-c1))/torch.norm(at - torch.linalg.cross(up,at-c1)) * torch.norm(at-c1)\n",
    "\n",
    "                # Use 90 degree rotated position, and same at and up vectors as image 1\n",
    "                R2 = look_at_rotation(c2[None, :],\n",
    "                                    model.camera_position[1][None, :],\n",
    "                                    model.camera_position[2][None, :], device=model.device)  # dimensions of tensor (1, 3, 3)\n",
    "                T2 = -torch.bmm(R2.transpose(1, 2), c2[None, :, None])[:, :, 0]  # dimensions of tensor (1, 3)\n",
    "\n",
    "                image2 = phong_renderer(meshes_world=model.meshes.clone(), R=R2, T=T2)\n",
    "                image2 = image2[0, ..., :3].detach().squeeze().cpu().numpy()\n",
    "                image2 = img_as_ubyte(image2)\n",
    "\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.imshow(image_sillouette2.detach().squeeze().cpu().numpy()[..., 3], cmap='viridis', alpha=0.5)\n",
    "                plt.imshow(model.image_ref2.detach().squeeze().cpu().numpy(), cmap='hot', alpha=0.5)\n",
    "                plt.title(\"LAT View\", size=fontsize)\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                # plt.savefig('../data/output/synthetic/temp_gif_frame_LAT.png', bbox_inches='tight')  # Save to a temporary location\n",
    "                # writer.append_data(imageio.v2.imread('../data/output/synthetic/temp_gif_frame_LAT.png'))  # Read and append to GIF\n",
    "\n",
    "                matplotlib.image.imsave(\n",
    "                \"../data/output/synthetic/image_sillouette2_synthetic_\"+str(index_for_gif)+\".png\",\n",
    "                image_sillouette2.detach().squeeze().cpu().numpy()[..., 3])\n",
    "\n",
    "            index_for_gif += 1\n",
    "    writer.close()\n",
    "    writer2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.loss_graph)\n",
    "plt.title('First Camera Loss by Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('First Camera Loss')\n",
    "plt.annotate('Second Camera Added', \n",
    "            xy=(second_camera_added_i, model.loss_graph[second_camera_added_i]+10), \n",
    "            xytext=(second_camera_added_i, model.loss_graph[second_camera_added_i]+30), \n",
    "            arrowprops = dict(facecolor='black', shrink=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute 6D, ADD Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6D Error\n",
    "print('6D Error:')\n",
    "print('Angular Difference (in degrees) = ' + \n",
    "      str(rut.angular_difference_error(R_0.squeeze().cpu().numpy(), R.cpu().detach().squeeze().numpy())))\n",
    "\n",
    "print('Translation Difference (in mm) = ' + \n",
    "      str(torch.norm(-torch.bmm(torch.linalg.inv(R_0.transpose(1, 2)), T_0.unsqueeze(2))\n",
    "                     -model.camera_position[0][None, :, None]).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD Error\n",
    "print('ADD Error (in mm) = ' + str(rut.ADD_Error(\n",
    "    \"./dsa_mesh_main_vessels_dec_centered_zy_change.obj\",\n",
    "    R_0.cpu().squeeze().numpy(),\n",
    "    T_0.cpu().squeeze().numpy(),\n",
    "    R.cpu().detach().squeeze().numpy(),\n",
    "    T.cpu().detach().squeeze().numpy())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
