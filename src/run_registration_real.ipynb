{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import registration_utils as rut\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch3d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image \n",
    "import imageio\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from skimage import color, img_as_ubyte\n",
    "from scipy.spatial.transform import Rotation\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex, look_at_view_transform, look_at_rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cuda device \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "    print(torch.cuda.is_available())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Reference Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSA AP Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsa_ap = rut.xray_read_dt('../data/input/2D_DSA_AP')\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(dsa_ap,  cmap='gray')\n",
    "plt.grid(False)\n",
    "\n",
    "print(dsa_ap.shape)\n",
    "\n",
    "# Resize image\n",
    "dsa_ap = rut.resize_image(dsa_ap, 1000)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(dsa_ap, cmap='gray')\n",
    "plt.title(\"Resized Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation of DSA AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ref = rut.DSA_segmentation(dsa_ap, sigma = 0.1)\n",
    "\n",
    "print(image_ref.shape)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_ref, cmap='gray')\n",
    "plt.grid(False)\n",
    "\n",
    "# Reversed image\n",
    "image_ref = 1-image_ref\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_ref, cmap='gray')\n",
    "plt.title(\"Inverted Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Lateral Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsa_lat = rut.xray_read_dt('../data/input/2D_DSA_LAT')\n",
    "\n",
    "plt.figure(figsize= (15, 15))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(dsa_lat,  cmap='gray')  # only plot the alpha channel of the RGBA image\n",
    "plt.grid(False)\n",
    "\n",
    "dsa_lat = rut.resize_image(dsa_lat, 1000)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(dsa_lat, cmap='gray')\n",
    "plt.title(\"Resized Image\")\n",
    "\n",
    "image_ref2 = rut.DSA_segmentation(dsa_lat, sigma = 1e-4) # type: ignore\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_ref2, cmap='gray')\n",
    "plt.grid(False)\n",
    "\n",
    "# 반전된 image\n",
    "image_ref2 = 1-image_ref2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_ref2, cmap='gray')\n",
    "plt.title(\"Inverted Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to save current segmented image\n",
    "\n",
    "# png_image = Image.fromarray(image_ref2.astype(np.uint8)*255)\n",
    "# png_image.save(/../data/input/DSA_LAT_seg.png')\n",
    "# png_image = Image.fromarray(image_ref.astype(np.uint8)*255)\n",
    "# png_image.save('../data/input/DSA_AP_seg.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input manually segmented images\n",
    "image_ref = (np.array(Image.open('../data/input/DSA_AP_seg.png'))/255.0).astype(np.uint8)\n",
    "image_ref2 = (np.array(Image.open('../data/input/DSA_LAT_seg.png'))/255.0).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the obj and ignore the textures and materials.\n",
    "verts, faces_idx, _ = load_obj(\"../data/input/dsa_mesh_zaxis_up_trimmed.obj\")\n",
    "faces = faces_idx.verts_idx\n",
    "\n",
    "# Initialize each vertex to be white in color.\n",
    "verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "textures = TexturesVertex(verts_features=verts_rgb.to(device))\n",
    "\n",
    "dsa_mesh = Meshes(\n",
    "    verts=[verts.to(device)],\n",
    "    faces=[faces.to(device)],\n",
    "    textures=textures\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a perspective camera.\n",
    "cameras = FoVPerspectiveCameras(device=device)\n",
    "\n",
    "# Set parameters which control the opacity and the sharpness of edges\n",
    "blend_params = BlendParams(sigma=1e-7, gamma=1e-5)\n",
    "\n",
    "# Define the settings for rasterization and shading. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size= image_ref.shape,\n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma,\n",
    "    faces_per_pixel=100,\n",
    "    bin_size=0\n",
    ")\n",
    "\n",
    "# Create a silhouette mesh renderer by composing a rasterizer and a shader.\n",
    "silhouette_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "# We will also create a Phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=image_ref.shape,\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    "    bin_size=0\n",
    ")\n",
    "# We can add a point light in front of the object.\n",
    "lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(device=device, cameras=cameras, lights=lights)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, meshes, renderer, image_ref, image_ref2, camera_initial_position):\n",
    "\n",
    "        super().__init__()\n",
    "        self.meshes = meshes\n",
    "        self.device = meshes.device\n",
    "        self.renderer = renderer\n",
    "\n",
    "        self.camera1_weight = 1.0\n",
    "        self.camera2_weight = 0.0\n",
    "\n",
    "        self.loss_graph = []\n",
    "\n",
    "        image_ref = torch.from_numpy(image_ref.astype(np.float32))\n",
    "        self.register_buffer('image_ref', image_ref)\n",
    "\n",
    "        image_ref2 = torch.from_numpy(image_ref2.astype(np.float32))\n",
    "        self.register_buffer('image_ref2', image_ref2)\n",
    "\n",
    "        # Create an optimizable parameter for the x, y, z position of the camera, at vector, and up vector\n",
    "        self.camera_position = nn.Parameter(\n",
    "            torch.from_numpy(np.array([\n",
    "                [camera_initial_position[0][0].item(), camera_initial_position[0][1].item(), camera_initial_position[0][2].item()],\n",
    "                [camera_initial_position[1][0].item(), camera_initial_position[1][1].item(), camera_initial_position[1][2].item()],\n",
    "                [camera_initial_position[2][0].item(), camera_initial_position[2][1].item(), camera_initial_position[2][2].item()]\n",
    "                ], dtype=np.float32)).to(meshes.device))\n",
    "\n",
    "    def forward(self):\n",
    "        # Render the image using the updated camera position. Based on the new position of the\n",
    "        # camera we calculate the rotation and translation matrices\n",
    "        # position, at, up\n",
    "        R = look_at_rotation(self.camera_position[0][None, :], \n",
    "                             self.camera_position[1][None, :],\n",
    "                             self.camera_position[2][None, :],device=self.device)\n",
    "        T = -torch.bmm(R.transpose(1, 2), self.camera_position[0][None, :, None])[:, :, 0]\n",
    "        \n",
    "        c1 = self.camera_position[0][None, :].squeeze()\n",
    "        at = self.camera_position[1][None, :].squeeze()\n",
    "        up = self.camera_position[2][None, :].squeeze()\n",
    "\n",
    "        # camera 2 position is rotated 90 degrees from camera 1\n",
    "        c2 = (at - torch.linalg.cross(up,at-c1))/torch.norm(at - torch.linalg.cross(up,at-c1)) * torch.norm(at-c1)\n",
    "\n",
    "        # Use 90 degree rotated position, and same at and up vectors as image 1\n",
    "        R2 = look_at_rotation(c2[None, :],\n",
    "                            self.camera_position[1][None, :],\n",
    "                            self.camera_position[2][None, :], device=self.device)\n",
    "        T2 = -torch.bmm(R2.transpose(1, 2), c2[None, :, None])[:, :, 0]\n",
    "\n",
    "        image = self.renderer(meshes_world=self.meshes.clone(), R=R, T=T)\n",
    "        image2 = self.renderer(meshes_world=self.meshes.clone(), R=R2, T=T2)\n",
    "\n",
    "        # Calculate the silhouette loss\n",
    "        loss = torch.sqrt(torch.sum(self.camera1_weight*(image[..., 3] - self.image_ref) ** 2 +\n",
    "                                    self.camera2_weight*(image2[..., 3] - self.image_ref2) ** 2))\n",
    "        loss_first_camera = torch.sqrt(torch.sum(self.camera1_weight*(image[..., 3] - self.image_ref) ** 2))\n",
    "        self.loss_graph.append(loss_first_camera.item())\n",
    "        return loss, image, image2, loss_first_camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save images periodically and compose them into a GIF.\n",
    "filename_output = \"../data/output/real/registration_real_AP.gif\"\n",
    "writer = imageio.get_writer(filename_output, mode='I')\n",
    "filename_output2 = \"../data/output/real/registration_real_LAT.gif\"\n",
    "writer2 = imageio.get_writer(filename_output2, mode='I')\n",
    "\n",
    "first_camera_initial_position = torch.tensor([[-23.187042236328125, -162.63189697265625, 5.946759223937988],\n",
    "[0,0,40],\n",
    "[-0.06765928119421005, 0.031012320891022682, 1.623045802116394]])\n",
    "\n",
    "# Initialize a model using the renderer, mesh and reference image\n",
    "model = Model(meshes=dsa_mesh,\n",
    "              renderer=silhouette_renderer,\n",
    "              image_ref=image_ref,\n",
    "              image_ref2=image_ref2,\n",
    "              camera_initial_position=first_camera_initial_position\n",
    "              ).to(device)\n",
    "\n",
    "# Create an optimizer. Here we are using Adam and we pass in the parameters of the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Starting Position and Reference Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "_, image_init, image_init2, _ = model()\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(image_init.detach().squeeze().cpu().numpy()[..., 3])\n",
    "plt.grid(False)\n",
    "plt.title(\"Starting position AP\")\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(model.image_ref.cpu().numpy().squeeze())\n",
    "plt.grid(False)\n",
    "plt.title(\"Reference silhouette AP\")\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(image_init2.detach().squeeze().cpu().numpy()[..., 3])\n",
    "plt.grid(False)\n",
    "plt.title(\"Starting position LAT\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(model.image_ref2.cpu().numpy().squeeze())\n",
    "plt.grid(False)\n",
    "plt.title(\"Reference silhouette LAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load untrimmmed mesh for visualization\n",
    "verts, faces_idx, _ = load_obj(\"../data/input/dsa_mesh_zaxis_up.obj\")\n",
    "faces = faces_idx.verts_idx\n",
    "verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "textures = TexturesVertex(verts_features=verts_rgb.to(device))\n",
    "\n",
    "dsa_mesh_for_gif = Meshes(\n",
    "    verts=[verts.to(device)],\n",
    "    faces=[faces.to(device)],\n",
    "    textures=textures\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strikes_to_break = 0\n",
    "strike_out = 10\n",
    "loss_previous1 = 0.0\n",
    "second_camera_added_i = 0\n",
    "second_registration_mode = False # True for both AP + LAT at the same time, False for AP then LAT\n",
    "if (second_registration_mode == True):\n",
    "    model.camera1_weight = 1.0\n",
    "    model.camera2_weight = 1.0\n",
    "total_iterations = 1000\n",
    "index_for_gif = 0\n",
    "fontsize = 20\n",
    "\n",
    "# Create output file for rotational and translation matrices\n",
    "with open('../data/output/real/RT_data_real.txt', 'w') as f:\n",
    "\n",
    "    loop = tqdm(range(total_iterations))\n",
    "    for i in loop:\n",
    "\n",
    "        # Breaks if loss increases or loss difference is small strikes_to_break number of times\n",
    "        if i >= 11 and i % 10 ==0:\n",
    "            if  loss.item() - loss_previous1 > 10:\n",
    "                strikes_to_break += 1\n",
    "                print(\"strike \"+str(strikes_to_break)+\" because loss increased at i = \"+str(i)+\".\")\n",
    "\n",
    "            if abs(loss.item() - loss_previous1) < 0.08:\n",
    "                strikes_to_break += 1\n",
    "                print(\"strike \"+str(strikes_to_break)+\" because loss difference is small at i = \"+str(i)+\".\")\n",
    "\n",
    "        if second_registration_mode == True:\n",
    "            if strikes_to_break == strike_out:\n",
    "                break\n",
    "        else:\n",
    "            if strikes_to_break == strike_out or i == int(total_iterations/2):  \n",
    "                print(\"Adding second image to registration.\")\n",
    "                model.camera1_weight = 1.0\n",
    "                model.camera2_weight = 1.0\n",
    "                strikes_to_break = 0\n",
    "                second_registration_mode = True\n",
    "                second_camera_added_i = i\n",
    "\n",
    "        if i >= 1 and i % 10 == 0:\n",
    "            loss_previous1 = loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, image_sillouette, image_sillouette2, loss_first_camera = model()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description('Optimizing (loss %.4f)' % loss.data)    \n",
    "            \n",
    "        #Save outputs to create a GIF.\n",
    "        if i % 10 == 0:\n",
    "            R = look_at_rotation(model.camera_position[0][None, :],\n",
    "                                model.camera_position[1][None, :],\n",
    "                                model.camera_position[2][None, :], device=model.device)\n",
    "            T = -torch.bmm(R.transpose(1, 2), model.camera_position[0][None, :, None])[:, :, 0]   # (1, 3)\n",
    "\n",
    "            image = phong_renderer(meshes_world=dsa_mesh_for_gif, R=R, T=T)\n",
    "            image = image[0, ..., :3].detach().squeeze().cpu().numpy()\n",
    "            image = img_as_ubyte(image)\n",
    "\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(color.rgb2gray(image), cmap='Greys', alpha=0.5)\n",
    "            plt.imshow(dsa_ap, cmap='gray', alpha=0.5)\n",
    "            plt.title(\"AP Views\", size=fontsize)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # plt.savefig('../data/output/real/temp_gif_frame_AP.png', bbox_inches='tight')  # Save to a temporary location\n",
    "            # writer.append_data(imageio.v2.imread('../data/output/real/temp_gif_frame_AP.png'))  # Read and append to GIF\n",
    "\n",
    "            f.write(str(model.camera_position[0][None, :].cpu().detach().squeeze().tolist()))\n",
    "            f.write('\\n')\n",
    "            f.write(str(model.camera_position[1][None, :].cpu().detach().squeeze().tolist()))\n",
    "            f.write('\\n')\n",
    "            f.write(str(model.camera_position[2][None, :].cpu().detach().squeeze().tolist()))\n",
    "            f.write('\\n')\n",
    "\n",
    "            if(second_registration_mode):\n",
    "                c1 = model.camera_position[0][None, :].squeeze()\n",
    "                at = model.camera_position[1][None, :].squeeze()\n",
    "                up = model.camera_position[2][None, :].squeeze()\n",
    "\n",
    "                c2 = (at - torch.linalg.cross(up,at-c1))/torch.norm(at - torch.linalg.cross(up,at-c1)) * torch.norm(at-c1)\n",
    "\n",
    "                # Use 90 degree rotated position, and same at and up vectors as image 1\n",
    "                R2 = look_at_rotation(c2[None, :],\n",
    "                                    model.camera_position[1][None, :],\n",
    "                                    model.camera_position[2][None, :], device=model.device)\n",
    "                T2 = -torch.bmm(R2.transpose(1, 2), c2[None, :, None])[:, :, 0]\n",
    "\n",
    "                image2 = phong_renderer(meshes_world=dsa_mesh_for_gif, R=R2, T=T2)\n",
    "                image2 = image2[0, ..., :3].detach().squeeze().cpu().numpy()\n",
    "                image2 = img_as_ubyte(image2)\n",
    "\n",
    "                # LAT\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.imshow(color.rgb2gray(image2), cmap='Greys', alpha=0.5)\n",
    "                plt.imshow(dsa_lat, cmap='gray', alpha=0.5)\n",
    "                plt.title(\"LAT Views\", size=fontsize)\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                # plt.savefig('../data/output/real/temp_gif_frame_LAT.png', bbox_inches='tight')  # Save to a temporary location\n",
    "                # writer.append_data(imageio.v2.imread('../data/output/real/temp_gif_frame_LAT.png'))  # Read and append to GIF\n",
    "            index_for_gif += 1\n",
    "    writer.close()\n",
    "    writer2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.loss_graph)\n",
    "plt.title('First Camera Loss by Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('First Camera Loss')\n",
    "plt.annotate('Second Camera Added', \n",
    "            xy=(second_camera_added_i, model.loss_graph[second_camera_added_i]+10), \n",
    "            xytext=(second_camera_added_i, model.loss_graph[second_camera_added_i]+30), \n",
    "            arrowprops = dict(facecolor='black', shrink=0.05))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
